- [[FAC研究]]
	- 大 FAC大失敗啊... 當我把 solve Q value linear system換成 partially solve only for actions that advantage $\geq$ 0，整個結果直接壞掉，甚至連底下這個 2x2的迷宮都解決不了
	- ![Figure_1.png](../assets/Figure_1_1756342216350_0.png)
	- 這邊紅色的是障礙物 (r=-50)，藍色的是 goal (r=1)。可以看到 state = 1 (右上)出了個包，policy並沒有選擇往下，而是停留在原地(action = up to bump into the wall)。這是由於在第一次更新後，$(1, down)$這個 state-action pair被認定有 $< 0$的 advantage，因此就不再更新 Q-value了。為什麼 $advantage(1, down) < 0$呢? 因為 state = 3在當時有可能會向左走撞到障礙物，導致他的 q-value非常低。
	- 經過這次的研究，我總算理解為什麼 off-policy演算法這麼無力了。你看喔，我要讓 $(1, down)$這個 state-action pair出現在我的 trajectory裡，要嘛就用 $Value(3)$來引導 Q$(1, down)$提升，要嘛就用 model-based的方法告訴我 $(1, down)$會把我引導到一個其實很棒的狀態 $3$。但，不管是哪種方式，少不了的都是「需要 $(1, down)$會把我引導到狀態 $3$的經驗」。
	- 問題是，這經驗去哪裡找? 很不情願地說， exploration是個很不錯的方法。但我不甘心! 這種東西怎麼可以交給運氣?! 有沒有機會，直接從狀態 $3$直接反推出它是由 $(1, down)$ (或 $(2, right)$)造成的? 幹，可是這也要你有過這方面的經驗才行。好吧...看來一切都是靠 exploration了，真的跑不掉......
	- 這時候你再回來看 policy gradient，只要 q-values不要差太多，那各個 action的機率會是相似的，這就可以幫助我累積到很足夠的經驗了。但就這點來看，SAC也做的到，而且更直接。
-